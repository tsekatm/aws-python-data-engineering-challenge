{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXgc6BCiIKeq"
      },
      "source": [
        "\n",
        "Click here to opern this notebook in Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tsekatm/aws-python-data-engineering-challenge/blob/main/load_shedding_analysis.ipynb)\n",
        "# South Africa Load Shedding Analysis\n",
        "## AWS Data Engineering Challenge\n",
        "\n",
        "**Author**: Tebogo Tseka  \n",
        "**Date**: 03 July 2025  \n",
        "**Time**: 19:00 SAST.  \n",
        "**Zoom Link**:  [bit.ly/3VmV3CK](https://bit.ly/3VmV3CK)  \n",
        "**Meetup Link / Register here**: [meetup.com/mzansi-aws](https://meetup.com/mzansi-aws)\n",
        "\n",
        "**Objective**: Analyze South Africa's load shedding patterns and demonstrate AWS S3 capabilities\n",
        "\n",
        "### Features\n",
        "- AWS S3 integration for data storage\n",
        "- Data analysis of electricity crisis patterns\n",
        "- Machine learning predictions\n",
        "- Interactive visualizations\n",
        "- Business intelligence insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjr54G4kIKes",
        "outputId": "fe76873f-0a92-4e30-9c26-0862b899bed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.39.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting botocore<1.40.0,>=1.39.1 (from boto3)\n",
            "  Downloading botocore-1.39.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.1->boto3) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading boto3-1.39.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.39.1-py3-none-any.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.39.1 botocore-1.39.1 jmespath-1.0.1 s3transfer-0.13.0\n",
            "✅ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Q1 Why do we always start with pip install?\n",
        "!pip install boto3 pandas numpy matplotlib seaborn plotly scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "# Powerful data manipulation and analysis library - think Excel but for programmers\n",
        "import pandas as pd\n",
        "\n",
        "# Numerical computing library for fast mathematical operations on arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# Basic plotting library for creating static charts, graphs, and visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Statistical visualization library built on matplotlib - makes beautiful plots with less code\n",
        "import seaborn as sns\n",
        "\n",
        "# Interactive visualization library for creating dynamic, web-ready charts and dashboards\n",
        "import plotly.express as px\n",
        "\n",
        "# Q2 Why do we use Boto3?\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError, NoCredentialsError\n",
        "\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Q3 What is SKLearn?\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cd13021"
      },
      "source": [
        "To configure AWS credentials in Google Colab, you can use the secrets manager. Click on the \"🔑\" icon in the left sidebar, and add your AWS Access Key ID and AWS Secret Access Key. Name them `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` respectively.\n",
        "\n",
        "Once you have added the secrets, you can access them in your notebook like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f194eeb"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "print(\"✅ AWS credentials configured!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103636c6"
      },
      "source": [
        "After running the cell above, you can run the next cell that initializes the AWSConfig class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54hxiS47IKet"
      },
      "outputs": [],
      "source": [
        "# AWS Configuration Class\n",
        "# Holds all configuration data for this notebook to be able to connect to AWS\n",
        "# Thing like region, bucket name\n",
        "# You can put more global config that you want to reuse in the notebook\n",
        "# Q4 Why is it a good idea to do this?\n",
        "class AWSConfig:\n",
        "    def __init__(self):\n",
        "        self.region = 'us-east-1'\n",
        "        # Use the correct bucket name from your S3 console\n",
        "        self.bucket_name = 'aws-python-data-engineering-challenge'\n",
        "\n",
        "        try:\n",
        "            self.s3_client = boto3.client('s3', region_name=self.region)\n",
        "            self.s3_client.list_buckets()\n",
        "            print(f\"✅ AWS S3 connected! Region: {self.region}\")\n",
        "            print(f\"🪣 Target bucket: {self.bucket_name}\")\n",
        "        except NoCredentialsError:\n",
        "            print(\"❌ AWS credentials not found. Please configure AWS CLI.\")\n",
        "            print(\"Run: aws configure\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ AWS error: {e}\")\n",
        "\n",
        "    def verify_bucket_access(self):\n",
        "        \"\"\"Verify we can access the existing bucket\"\"\"\n",
        "        try:\n",
        "            self.s3_client.head_bucket(Bucket=self.bucket_name)\n",
        "            print(f\"✅ Successfully connected to existing bucket: {self.bucket_name}\")\n",
        "            return True\n",
        "        except ClientError as e:\n",
        "            error_code = e.response['Error']['Code']\n",
        "            if error_code == '404':\n",
        "                print(f\"❌ Bucket not found: {self.bucket_name}\")\n",
        "            elif error_code == '403':\n",
        "                print(f\"❌ Access denied to bucket: {self.bucket_name}\")\n",
        "            else:\n",
        "                print(f\"❌ Error accessing bucket: {e}\")\n",
        "            return False\n",
        "\n",
        "# Initialize AWS (connects to existing bucket)\n",
        "aws = AWSConfig()\n",
        "aws.verify_bucket_access()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jrouA0UIKet"
      },
      "outputs": [],
      "source": [
        "# S3 Manager Class\n",
        "# Manages interactions with S3 using the above config\n",
        "# Q5 Why do we need this class?\n",
        "# Q6 Based on the code below what does this class do?\n",
        "class S3Manager:\n",
        "    def __init__(self, aws_config):\n",
        "        self.s3 = aws_config.s3_client\n",
        "        self.bucket = aws_config.bucket_name\n",
        "\n",
        "    def upload_df(self, df, key):\n",
        "        try:\n",
        "            csv_data = df.to_csv(index=False)\n",
        "            #This line uploads the CSV above to S3 using S3 client.\n",
        "            self.s3.put_object(\n",
        "                Bucket=self.bucket,\n",
        "                Key=key,\n",
        "                Body=csv_data.encode('utf-8')\n",
        "            )\n",
        "            print(f\"✅ Uploaded: {key}\")\n",
        "            return f\"s3://{self.bucket}/{key}\"\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Upload failed: {e}\")\n",
        "\n",
        "    def download_df(self, key):\n",
        "        try:\n",
        "            obj = self.s3.get_object(Bucket=self.bucket, Key=key)\n",
        "            df = pd.read_csv(obj['Body'])\n",
        "            print(f\"✅ Downloaded: {key}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Download failed: {e}\")\n",
        "\n",
        "    def list_files(self):\n",
        "        try:\n",
        "            response = self.s3.list_objects_v2(Bucket=self.bucket)\n",
        "            if 'Contents' in response:\n",
        "                files = [obj['Key'] for obj in response['Contents']]\n",
        "                print(f\"📁 Files in {self.bucket}:\")\n",
        "                for f in files:\n",
        "                    print(f\"  📄 {f}\")\n",
        "                return files\n",
        "            else:\n",
        "                print(\"📁 Bucket is empty\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"❌ List failed: {e}\")\n",
        "\n",
        "# Initialize S3 Manager\n",
        "s3_mgr = S3Manager(aws)\n",
        "print(\"🔧 S3 Manager ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-duKUCjIKet"
      },
      "outputs": [],
      "source": [
        "# Generate sample load shedding data\n",
        "# Do not run this code. It is here just in cases the next code block does not work.\n",
        "print(\"🔧 Creating sample data...\")\n",
        "\n",
        "#\n",
        "np.random.seed(42)\n",
        "\n",
        "#\n",
        "dates = pd.date_range('2015-01-01', '2020-12-31', freq='6H')\n",
        "\n",
        "# Create realistic load shedding patterns\n",
        "loadshedding_data = pd.DataFrame({\n",
        "    'created_at': dates,\n",
        "    'stage': np.random.choice([0, 1, 2, 3, 4], len(dates), p=[0.6, 0.2, 0.1, 0.07, 0.03])\n",
        "})\n",
        "\n",
        "# Add seasonal patterns (more load shedding in winter)\n",
        "winter_mask = loadshedding_data['created_at'].dt.month.isin([6, 7, 8])\n",
        "loadshedding_data.loc[winter_mask, 'stage'] = np.minimum(\n",
        "    loadshedding_data.loc[winter_mask, 'stage'] +\n",
        "    np.random.binomial(1, 0.3, winter_mask.sum()), 4\n",
        ")\n",
        "\n",
        "# Add peak hour patterns\n",
        "peak_hours = loadshedding_data['created_at'].dt.hour.isin([6, 7, 8, 17, 18, 19])\n",
        "loadshedding_data.loc[peak_hours, 'stage'] = np.minimum(\n",
        "    loadshedding_data.loc[peak_hours, 'stage'] +\n",
        "    np.random.binomial(1, 0.2, peak_hours.sum()), 4\n",
        ")\n",
        "\n",
        "print(f\"✅ Generated {len(loadshedding_data)} load shedding records\")\n",
        "print(\"\\n📊 Sample data:\")\n",
        "print(loadshedding_data.head())\n",
        "print(f\"\\nDate range: {loadshedding_data['created_at'].min()} to {loadshedding_data['created_at'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Data Sources\n",
        "- **Load Shedding Data**: Kaggle / Eskom Data Portal\n",
        "- **Electricity Production**: FRED Economic Data (Series: PRENEL01ZAQ656N)  \n",
        "- **World Indicators**: World Bank Open Data\n",
        "- **Time Period**: 2015-2020 (Load Shedding), 1985-2018 (Production)"
      ],
      "metadata": {
        "id": "Dib3xFqdAZG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1VlBPozIKet"
      },
      "outputs": [],
      "source": [
        "# Download data from S3 (using correct bucket)\n",
        "# I uploaded this data to S3 by hand from the following sites: Kaggle - South Africa Load Shedding Dataset, https://www.eskom.co.za/dataportal/,  FRED Economic Data (Federal Reserve)\n",
        "# Before runnining thos code block make sure that:\n",
        "# 1. You have created the s3 bucket\n",
        "# 2. Uploaded the files to it.\n",
        "# Q7 Why should we do the downlaod from S3?\n",
        "print(\"📥 Loading datasets from S3...\")\n",
        "\n",
        "try:\n",
        "    # Files are in the raw-data/ folder based on your screenshot\n",
        "    print(\"📊 Downloading from raw-data/ folder...\")\n",
        "    loadshedding_df = s3_mgr.download_df('raw-data/south_africa_load_shedding_history.csv')\n",
        "    electricity_df = s3_mgr.download_df('raw-data/Total_Electricity_Production.csv')\n",
        "    world_indicators_df = s3_mgr.download_df('raw-data/world_indicators.csv')\n",
        "\n",
        "    if all(df is not None for df in [loadshedding_df, electricity_df, world_indicators_df]):\n",
        "        print(\"\\n✅ All datasets successfully downloaded!\")\n",
        "        print(f\"📊 Load shedding: {loadshedding_df.shape[0]} rows, {loadshedding_df.shape[1]} columns\")\n",
        "        print(f\"⚡ Electricity: {electricity_df.shape[0]} rows, {electricity_df.shape[1]} columns\")\n",
        "        print(f\"🌍 World indicators: {world_indicators_df.shape[0]} rows, {world_indicators_df.shape[1]} columns\")\n",
        "    else:\n",
        "        print(\"❌ Some downloads failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Download error: {e}\")\n",
        "\n",
        "# Verify bucket contents\n",
        "print(\"\\n🔍 Bucket structure:\")\n",
        "s3_mgr.list_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3NFVvpKIKeu"
      },
      "outputs": [],
      "source": [
        "# Data processing and feature engineering\n",
        "# Q8 What is feature engineering?\n",
        "# Q9 Why do we need it?\n",
        "def process_data(df):\n",
        "    processed = df.copy()\n",
        "    processed['created_at'] = pd.to_datetime(processed['created_at'])\n",
        "\n",
        "    # Time features\n",
        "    processed['year'] = processed['created_at'].dt.year\n",
        "    processed['month'] = processed['created_at'].dt.month\n",
        "    processed['day'] = processed['created_at'].dt.day\n",
        "    processed['hour'] = processed['created_at'].dt.hour\n",
        "    processed['day_of_week'] = processed['created_at'].dt.dayofweek\n",
        "    processed['quarter'] = processed['created_at'].dt.quarter\n",
        "\n",
        "    # Seasonal features\n",
        "    def get_season(month):\n",
        "        if month in [12, 1, 2]: return 'Summer'\n",
        "        elif month in [3, 4, 5]: return 'Autumn'\n",
        "        elif month in [6, 7, 8]: return 'Winter'\n",
        "        else: return 'Spring'\n",
        "\n",
        "    processed['season'] = processed['month'].apply(get_season)\n",
        "\n",
        "    # Binary features\n",
        "    processed['is_active'] = (processed['stage'] > 0).astype(int)\n",
        "    processed['is_weekend'] = (processed['day_of_week'] >= 5).astype(int)\n",
        "    processed['is_business_hours'] = ((processed['hour'] >= 8) & (processed['hour'] <= 17)).astype(int)\n",
        "\n",
        "    # Severity categories\n",
        "    def get_severity(stage):\n",
        "        if stage == 0: return 'None'\n",
        "        elif stage <= 2: return 'Moderate'\n",
        "        elif stage <= 4: return 'Severe'\n",
        "        else: return 'Critical'\n",
        "\n",
        "    processed['severity'] = processed['stage'].apply(get_severity)\n",
        "\n",
        "    return processed\n",
        "\n",
        "# Process the data - use the correct variable name\n",
        "# Uses above feature engineering method\n",
        "processed_data = process_data(loadshedding_df)  # Changed from loadshedding_data to loadshedding_df\n",
        "\n",
        "print(\"🔄 Data processed successfully!\")\n",
        "print(f\"📊 Shape: {processed_data.shape}\")\n",
        "print(\"\\n📋 New features:\")\n",
        "new_cols = [c for c in processed_data.columns if c not in loadshedding_df.columns]  # Also updated here\n",
        "print(new_cols)\n",
        "\n",
        "print(\"\\n📊 Sample processed data:\")\n",
        "print(processed_data[['created_at', 'stage', 'season', 'severity', 'is_active']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qX6bJiwIKeu"
      },
      "outputs": [],
      "source": [
        "# Upload processed data to S3\n",
        "# Q10 Why?\n",
        "processed_path = s3_mgr.upload_df(processed_data, f'processed/loadshedding_{timestamp}.csv')\n",
        "print(f\"📋 Processed data path: {processed_path}\")\n",
        "\n",
        "print(\"\\n✅ Data processing and upload completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbT45jl2IKeu"
      },
      "outputs": [],
      "source": [
        "# Data visualization and analysis\n",
        "# Q11 Why?\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('South Africa Load Shedding Analysis (2015-2020)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Events by year\n",
        "yearly = processed_data.groupby('year')['stage'].count()\n",
        "yearly.plot(kind='bar', ax=axes[0,0], color='red', alpha=0.7)\n",
        "axes[0,0].set_title('Load Shedding Events by Year')\n",
        "axes[0,0].set_ylabel('Number of Events')\n",
        "\n",
        "# 2. Stage distribution\n",
        "stage_dist = processed_data['stage'].value_counts().sort_index()\n",
        "stage_dist.plot(kind='bar', ax=axes[0,1], color='orange', alpha=0.7)\n",
        "axes[0,1].set_title('Distribution by Stage')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "\n",
        "# 3. Monthly patterns\n",
        "monthly = processed_data.groupby('month')['stage'].count()\n",
        "monthly.plot(kind='bar', ax=axes[1,0], color='blue', alpha=0.7)\n",
        "axes[1,0].set_title('Events by Month')\n",
        "axes[1,0].set_ylabel('Number of Events')\n",
        "\n",
        "# 4. Severity over time\n",
        "monthly_severity = processed_data.groupby(['year', 'month'])['stage'].mean().reset_index()\n",
        "monthly_severity['date'] = pd.to_datetime(monthly_severity[['year', 'month']].assign(day=1))\n",
        "axes[1,1].plot(monthly_severity['date'], monthly_severity['stage'], marker='o', color='green')\n",
        "axes[1,1].set_title('Average Stage Over Time')\n",
        "axes[1,1].set_ylabel('Average Stage')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Key insights\n",
        "print('🔍 Key Insights:')\n",
        "print(f'📊 Total events: {len(processed_data):,}')\n",
        "print(f'⚡ Active events: {processed_data[\"is_active\"].sum():,}')\n",
        "print(f'🔴 Max stage: {processed_data[\"stage\"].max()}')\n",
        "print(f'📈 Peak year: {yearly.idxmax()} ({yearly.max()} events)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scLOsZlmIKeu"
      },
      "outputs": [],
      "source": [
        "# Machine Learning Model\n",
        "# Q12 Why?\n",
        "# Q13 Read the below code and explain what is does\n",
        "print('🤖 Training ML model...')\n",
        "\n",
        "# Creates a list of column names that will be used as input features (predictors) for the model\n",
        "# These are the variables the model will use to predict the load shedding stage\n",
        "features = ['year', 'month', 'day', 'hour', 'day_of_week', 'quarter', 'is_weekend', 'is_business_hours']\n",
        "X = processed_data[features]\n",
        "y = processed_data['stage']\n",
        "\n",
        "\n",
        "# Splits the data into training and testing sets:\n",
        "# - 80% for training the model (X_train, y_train)\n",
        "# - 20% for testing the model's performance (X_test, y_test)\n",
        "# - random_state=42 ensures reproducible results (same split every time)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Creates a StandardScaler object that will normalize features to have mean=0 and std=1\n",
        "# This ensures all features are on the same scale (important for many ML algorithms)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "# Creates a Random Forest model with:\n",
        "# - n_estimators=100: Uses 100 decision trees\n",
        "# - random_state=42: Ensures reproducible results\n",
        "# - RandomForestRegressor: Used for predicting continuous values (load shedding stages)\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# Trains the model using the scaled training features (X_train_scaled) and targets (y_train)\n",
        "# The model learns patterns between features and load shedding stages\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "# Uses the trained model to predict load shedding stages for the test data\n",
        "# These predictions will be compared to actual values to evaluate performance\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "# Calculates Mean Squared Error: average of squared differences between actual and predicted values\n",
        "# Lower MSE = better model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Displays the calculated performance metrics\n",
        "# .4f formats numbers to 4 decimal places\n",
        "print(f'📊 Model Performance:')\n",
        "print(f'  MSE: {mse:.4f}')\n",
        "print(f'  R²: {r2:.4f}')\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "# Creates a DataFrame showing which features are most important for predictions\n",
        "# Random Forest provides feature_importances_ showing how much each feature contributes\n",
        "# sort_values orders features from most to least important\n",
        "importance = pd.DataFrame({\n",
        "    'feature': features,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print('\\n📈 Feature Importance:')\n",
        "for _, row in importance.iterrows():\n",
        "    print(f'  {row[\"feature\"]}: {row[\"importance\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zi7oLc4IKeu"
      },
      "outputs": [],
      "source": [
        "# Test S3 data retrieval\n",
        "print('📥 Testing S3 data retrieval...')\n",
        "\n",
        "# Download data from S3\n",
        "s3_data = s3_mgr.download_df(f'processed/loadshedding_{timestamp}.csv')\n",
        "\n",
        "print(f'✅ Retrieved {len(s3_data)} rows from S3')\n",
        "print('Sample:')\n",
        "print(s3_data.head(3))\n",
        "\n",
        "# Data integrity check\n",
        "match = processed_data.equals(s3_data)\n",
        "print(f'\\n✅ Data integrity: {\"PASSED\" if match else \"FAILED\"}')\n",
        "\n",
        "# List all files\n",
        "print('\\n📁 All S3 files:')\n",
        "s3_mgr.list_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG0DtPtwIKev"
      },
      "outputs": [],
      "source": [
        "# Interactive visualization\n",
        "print('📊 Creating interactive charts...')\n",
        "\n",
        "# Time series plot\n",
        "fig = px.line(\n",
        "    monthly_severity,\n",
        "    x='date',\n",
        "    y='stage',\n",
        "    title='Load Shedding Severity Over Time',\n",
        "    labels={'stage': 'Average Stage', 'date': 'Date'}\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Heatmap by hour and day\n",
        "heatmap_data = processed_data.groupby(['hour', 'day_of_week'])['stage'].mean().reset_index()\n",
        "heatmap_pivot = heatmap_data.pivot(index='hour', columns='day_of_week', values='stage')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(heatmap_pivot, annot=True, cmap='Reds', fmt='.2f')\n",
        "plt.title('Average Load Shedding by Hour and Day')\n",
        "plt.xlabel('Day (0=Monday, 6=Sunday)')\n",
        "plt.ylabel('Hour')\n",
        "plt.show()\n",
        "\n",
        "# Seasonal analysis\n",
        "seasonal = processed_data.groupby('season').agg({\n",
        "    'stage': ['mean', 'max', 'count'],\n",
        "    'is_active': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "print('🌍 Seasonal Analysis:')\n",
        "print(seasonal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCbvxmslIKev"
      },
      "outputs": [],
      "source": [
        "# Save model metrics to S3\n",
        "print('💾 Saving model artifacts...')\n",
        "\n",
        "metrics = {\n",
        "    'timestamp': timestamp,\n",
        "    'model': 'RandomForestRegressor',\n",
        "    'mse': float(mse),\n",
        "    'r2': float(r2),\n",
        "    'features': features,\n",
        "    'train_size': len(X_train),\n",
        "    'test_size': len(X_test)\n",
        "}\n",
        "\n",
        "# Upload metrics\n",
        "metrics_json = json.dumps(metrics, indent=2)\n",
        "try:\n",
        "    aws.s3_client.put_object(\n",
        "        Bucket=aws.bucket_name,\n",
        "        Key=f'models/metrics_{timestamp}.json',\n",
        "        Body=metrics_json.encode('utf-8'),\n",
        "        ContentType='application/json'\n",
        "    )\n",
        "    print('✅ Model metrics saved to S3')\n",
        "except Exception as e:\n",
        "    print(f'❌ Save failed: {e}')\n",
        "\n",
        "print('\\n🎉 Analysis complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhLJRkAhIKev"
      },
      "outputs": [],
      "source": [
        "# Executive Summary\n",
        "print('📊 EXECUTIVE SUMMARY')\n",
        "print('='*50)\n",
        "\n",
        "total = len(processed_data)\n",
        "active = processed_data['is_active'].sum()\n",
        "max_stage = processed_data['stage'].max()\n",
        "date_range = f\"{processed_data['created_at'].min().year}-{processed_data['created_at'].max().year}\"\n",
        "\n",
        "print('🔍 KEY FINDINGS:')\n",
        "print(f'  📊 Total events: {total:,}')\n",
        "print(f'  ⚡ Active events: {active:,}')\n",
        "print(f'  🔴 Max stage: {max_stage}')\n",
        "print(f'  📅 Period: {date_range}')\n",
        "print(f'  📈 Peak year: {yearly.idxmax()}')\n",
        "print()\n",
        "\n",
        "print('☁️ AWS ACHIEVEMENTS:')\n",
        "print('  ✅ S3 data pipeline implemented')\n",
        "print('  ✅ Raw and processed data uploaded')\n",
        "print('  ✅ Bi-directional data flow demonstrated')\n",
        "print('  ✅ Data versioning with timestamps')\n",
        "print()\n",
        "\n",
        "print('🤖 ML INSIGHTS:')\n",
        "print(f'  📈 R² Score: {r2:.4f}')\n",
        "print(f'  🎯 Top feature: {importance.iloc[0][\"feature\"]}')\n",
        "print(f'  📊 Training samples: {len(X_train)}')\n",
        "print()\n",
        "\n",
        "print('💡 RECOMMENDATIONS:')\n",
        "print('  🔋 Focus on peak crisis periods')\n",
        "print('  ⏰ Optimize maintenance timing')\n",
        "print('  📊 Real-time monitoring system')\n",
        "print('  ☁️ Scale AWS infrastructure')\n",
        "print()\n",
        "\n",
        "print('🚀 NEXT STEPS:')\n",
        "print('  1. AWS Lambda automation')\n",
        "print('  2. QuickSight dashboards')\n",
        "print('  3. SageMaker deployment')\n",
        "print('  4. CloudWatch alerts')\n",
        "print()\n",
        "\n",
        "print('✅ NOTEBOOK READY FOR GOOGLE COLAB!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework - next Steps\n",
        "Well Architected data pipeline\n",
        "\n",
        "Daigram here\n",
        "\n",
        "Training Job\n",
        "S3 -> Sagemake Processing Job -> Sagamate training -> Model -> Deploy model\n",
        "\n",
        "Inference Job:\n",
        "S3 -> Sagemake Processing Job -> Inference -> Store Data in S3 -> Create Visualisation in QS\n",
        "\n",
        "#Thank you!!"
      ],
      "metadata": {
        "id": "Zdh99E3eYrj9"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}