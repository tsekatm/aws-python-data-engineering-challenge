{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXgc6BCiIKeq"
      },
      "source": [
        "\n",
        "Click here to opern this notebook in Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tsekatm/aws-python-data-engineering-challenge/blob/main/load_shedding_analysis.ipynb)\n",
        "# South Africa Load Shedding Analysis\n",
        "## AWS Data Engineering Challenge\n",
        "\n",
        "**Author**: Tebogo Tseka  \n",
        "**Date**: 03 July 2025  \n",
        "**Time**: 19:00 SAST.  \n",
        "**Zoom Link**:  [bit.ly/3VmV3CK](https://bit.ly/3VmV3CK)  \n",
        "**Meetup Link / Register here**: [meetup.com/mzansi-aws](https://meetup.com/mzansi-aws)\n",
        "\n",
        "**Objective**: Analyze South Africa's load shedding patterns and demonstrate AWS S3 capabilities\n",
        "\n",
        "### Features\n",
        "- AWS S3 integration for data storage\n",
        "- Data analysis of electricity crisis patterns\n",
        "- Machine learning predictions\n",
        "- Interactive visualizations\n",
        "- Business intelligence insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjr54G4kIKes",
        "outputId": "fe76873f-0a92-4e30-9c26-0862b899bed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.39.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting botocore<1.40.0,>=1.39.1 (from boto3)\n",
            "  Downloading botocore-1.39.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.1->boto3) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading boto3-1.39.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.39.1-py3-none-any.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.39.1 botocore-1.39.1 jmespath-1.0.1 s3transfer-0.13.0\n",
            "âœ… All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Q1 Why do we always start with pip install?\n",
        "!pip install boto3 pandas numpy matplotlib seaborn plotly scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "# Powerful data manipulation and analysis library - think Excel but for programmers\n",
        "import pandas as pd\n",
        "\n",
        "# Numerical computing library for fast mathematical operations on arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# Basic plotting library for creating static charts, graphs, and visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Statistical visualization library built on matplotlib - makes beautiful plots with less code\n",
        "import seaborn as sns\n",
        "\n",
        "# Interactive visualization library for creating dynamic, web-ready charts and dashboards\n",
        "import plotly.express as px\n",
        "\n",
        "# Q2 Why do we use Boto3?\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError, NoCredentialsError\n",
        "\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Q3 What is SKLearn?\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cd13021"
      },
      "source": [
        "To configure AWS credentials in Google Colab, you can use the secrets manager. Click on the \"ðŸ”‘\" icon in the left sidebar, and add your AWS Access Key ID and AWS Secret Access Key. Name them `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` respectively.\n",
        "\n",
        "Once you have added the secrets, you can access them in your notebook like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f194eeb"
      },
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "print(\"âœ… AWS credentials configured!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103636c6"
      },
      "source": [
        "After running the cell above, you can run the next cell that initializes the AWSConfig class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54hxiS47IKet"
      },
      "outputs": [],
      "source": [
        "# AWS Configuration Class\n",
        "# Holds all configuration data for this notebook to be able to connect to AWS\n",
        "# Thing like region, bucket name\n",
        "# You can put more global config that you want to reuse in the notebook\n",
        "# Q4 Why is it a good idea to do this?\n",
        "class AWSConfig:\n",
        "    def __init__(self):\n",
        "        self.region = 'us-east-1'\n",
        "        # Use the correct bucket name from your S3 console\n",
        "        self.bucket_name = 'aws-python-data-engineering-challenge'\n",
        "\n",
        "        try:\n",
        "            self.s3_client = boto3.client('s3', region_name=self.region)\n",
        "            self.s3_client.list_buckets()\n",
        "            print(f\"âœ… AWS S3 connected! Region: {self.region}\")\n",
        "            print(f\"ðŸª£ Target bucket: {self.bucket_name}\")\n",
        "        except NoCredentialsError:\n",
        "            print(\"âŒ AWS credentials not found. Please configure AWS CLI.\")\n",
        "            print(\"Run: aws configure\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ AWS error: {e}\")\n",
        "\n",
        "    def verify_bucket_access(self):\n",
        "        \"\"\"Verify we can access the existing bucket\"\"\"\n",
        "        try:\n",
        "            self.s3_client.head_bucket(Bucket=self.bucket_name)\n",
        "            print(f\"âœ… Successfully connected to existing bucket: {self.bucket_name}\")\n",
        "            return True\n",
        "        except ClientError as e:\n",
        "            error_code = e.response['Error']['Code']\n",
        "            if error_code == '404':\n",
        "                print(f\"âŒ Bucket not found: {self.bucket_name}\")\n",
        "            elif error_code == '403':\n",
        "                print(f\"âŒ Access denied to bucket: {self.bucket_name}\")\n",
        "            else:\n",
        "                print(f\"âŒ Error accessing bucket: {e}\")\n",
        "            return False\n",
        "\n",
        "# Initialize AWS (connects to existing bucket)\n",
        "aws = AWSConfig()\n",
        "aws.verify_bucket_access()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jrouA0UIKet"
      },
      "outputs": [],
      "source": [
        "# S3 Manager Class\n",
        "# Manages interactions with S3 using the above config\n",
        "# Q5 Why do we need this class?\n",
        "# Q6 Based on the code below what does this class do?\n",
        "class S3Manager:\n",
        "    def __init__(self, aws_config):\n",
        "        self.s3 = aws_config.s3_client\n",
        "        self.bucket = aws_config.bucket_name\n",
        "\n",
        "    def upload_df(self, df, key):\n",
        "        try:\n",
        "            csv_data = df.to_csv(index=False)\n",
        "            #This line uploads the CSV above to S3 using S3 client.\n",
        "            self.s3.put_object(\n",
        "                Bucket=self.bucket,\n",
        "                Key=key,\n",
        "                Body=csv_data.encode('utf-8')\n",
        "            )\n",
        "            print(f\"âœ… Uploaded: {key}\")\n",
        "            return f\"s3://{self.bucket}/{key}\"\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Upload failed: {e}\")\n",
        "\n",
        "    def download_df(self, key):\n",
        "        try:\n",
        "            obj = self.s3.get_object(Bucket=self.bucket, Key=key)\n",
        "            df = pd.read_csv(obj['Body'])\n",
        "            print(f\"âœ… Downloaded: {key}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Download failed: {e}\")\n",
        "\n",
        "    def list_files(self):\n",
        "        try:\n",
        "            response = self.s3.list_objects_v2(Bucket=self.bucket)\n",
        "            if 'Contents' in response:\n",
        "                files = [obj['Key'] for obj in response['Contents']]\n",
        "                print(f\"ðŸ“ Files in {self.bucket}:\")\n",
        "                for f in files:\n",
        "                    print(f\"  ðŸ“„ {f}\")\n",
        "                return files\n",
        "            else:\n",
        "                print(\"ðŸ“ Bucket is empty\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ List failed: {e}\")\n",
        "\n",
        "# Initialize S3 Manager\n",
        "s3_mgr = S3Manager(aws)\n",
        "print(\"ðŸ”§ S3 Manager ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-duKUCjIKet"
      },
      "outputs": [],
      "source": [
        "# Generate sample load shedding data\n",
        "# Do not run this code. It is here just in cases the next code block does not work.\n",
        "print(\"ðŸ”§ Creating sample data...\")\n",
        "\n",
        "#\n",
        "np.random.seed(42)\n",
        "\n",
        "#\n",
        "dates = pd.date_range('2015-01-01', '2020-12-31', freq='6H')\n",
        "\n",
        "# Create realistic load shedding patterns\n",
        "loadshedding_data = pd.DataFrame({\n",
        "    'created_at': dates,\n",
        "    'stage': np.random.choice([0, 1, 2, 3, 4], len(dates), p=[0.6, 0.2, 0.1, 0.07, 0.03])\n",
        "})\n",
        "\n",
        "# Add seasonal patterns (more load shedding in winter)\n",
        "winter_mask = loadshedding_data['created_at'].dt.month.isin([6, 7, 8])\n",
        "loadshedding_data.loc[winter_mask, 'stage'] = np.minimum(\n",
        "    loadshedding_data.loc[winter_mask, 'stage'] +\n",
        "    np.random.binomial(1, 0.3, winter_mask.sum()), 4\n",
        ")\n",
        "\n",
        "# Add peak hour patterns\n",
        "peak_hours = loadshedding_data['created_at'].dt.hour.isin([6, 7, 8, 17, 18, 19])\n",
        "loadshedding_data.loc[peak_hours, 'stage'] = np.minimum(\n",
        "    loadshedding_data.loc[peak_hours, 'stage'] +\n",
        "    np.random.binomial(1, 0.2, peak_hours.sum()), 4\n",
        ")\n",
        "\n",
        "print(f\"âœ… Generated {len(loadshedding_data)} load shedding records\")\n",
        "print(\"\\nðŸ“Š Sample data:\")\n",
        "print(loadshedding_data.head())\n",
        "print(f\"\\nDate range: {loadshedding_data['created_at'].min()} to {loadshedding_data['created_at'].max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Data Sources\n",
        "- **Load Shedding Data**: Kaggle / Eskom Data Portal\n",
        "- **Electricity Production**: FRED Economic Data (Series: PRENEL01ZAQ656N)  \n",
        "- **World Indicators**: World Bank Open Data\n",
        "- **Time Period**: 2015-2020 (Load Shedding), 1985-2018 (Production)"
      ],
      "metadata": {
        "id": "Dib3xFqdAZG_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1VlBPozIKet"
      },
      "outputs": [],
      "source": [
        "# Download data from S3 (using correct bucket)\n",
        "# I uploaded this data to S3 by hand from the following sites: Kaggle - South Africa Load Shedding Dataset, https://www.eskom.co.za/dataportal/,  FRED Economic Data (Federal Reserve)\n",
        "# Before runnining thos code block make sure that:\n",
        "# 1. You have created the s3 bucket\n",
        "# 2. Uploaded the files to it.\n",
        "# Q7 Why should we do the downlaod from S3?\n",
        "print(\"ðŸ“¥ Loading datasets from S3...\")\n",
        "\n",
        "try:\n",
        "    # Files are in the raw-data/ folder based on your screenshot\n",
        "    print(\"ðŸ“Š Downloading from raw-data/ folder...\")\n",
        "    loadshedding_df = s3_mgr.download_df('raw-data/south_africa_load_shedding_history.csv')\n",
        "    electricity_df = s3_mgr.download_df('raw-data/Total_Electricity_Production.csv')\n",
        "    world_indicators_df = s3_mgr.download_df('raw-data/world_indicators.csv')\n",
        "\n",
        "    if all(df is not None for df in [loadshedding_df, electricity_df, world_indicators_df]):\n",
        "        print(\"\\nâœ… All datasets successfully downloaded!\")\n",
        "        print(f\"ðŸ“Š Load shedding: {loadshedding_df.shape[0]} rows, {loadshedding_df.shape[1]} columns\")\n",
        "        print(f\"âš¡ Electricity: {electricity_df.shape[0]} rows, {electricity_df.shape[1]} columns\")\n",
        "        print(f\"ðŸŒ World indicators: {world_indicators_df.shape[0]} rows, {world_indicators_df.shape[1]} columns\")\n",
        "    else:\n",
        "        print(\"âŒ Some downloads failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Download error: {e}\")\n",
        "\n",
        "# Verify bucket contents\n",
        "print(\"\\nðŸ” Bucket structure:\")\n",
        "s3_mgr.list_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3NFVvpKIKeu"
      },
      "outputs": [],
      "source": [
        "# Data processing and feature engineering\n",
        "# Q8 What is feature engineering?\n",
        "# Q9 Why do we need it?\n",
        "def process_data(df):\n",
        "    processed = df.copy()\n",
        "    processed['created_at'] = pd.to_datetime(processed['created_at'])\n",
        "\n",
        "    # Time features\n",
        "    processed['year'] = processed['created_at'].dt.year\n",
        "    processed['month'] = processed['created_at'].dt.month\n",
        "    processed['day'] = processed['created_at'].dt.day\n",
        "    processed['hour'] = processed['created_at'].dt.hour\n",
        "    processed['day_of_week'] = processed['created_at'].dt.dayofweek\n",
        "    processed['quarter'] = processed['created_at'].dt.quarter\n",
        "\n",
        "    # Seasonal features\n",
        "    def get_season(month):\n",
        "        if month in [12, 1, 2]: return 'Summer'\n",
        "        elif month in [3, 4, 5]: return 'Autumn'\n",
        "        elif month in [6, 7, 8]: return 'Winter'\n",
        "        else: return 'Spring'\n",
        "\n",
        "    processed['season'] = processed['month'].apply(get_season)\n",
        "\n",
        "    # Binary features\n",
        "    processed['is_active'] = (processed['stage'] > 0).astype(int)\n",
        "    processed['is_weekend'] = (processed['day_of_week'] >= 5).astype(int)\n",
        "    processed['is_business_hours'] = ((processed['hour'] >= 8) & (processed['hour'] <= 17)).astype(int)\n",
        "\n",
        "    # Severity categories\n",
        "    def get_severity(stage):\n",
        "        if stage == 0: return 'None'\n",
        "        elif stage <= 2: return 'Moderate'\n",
        "        elif stage <= 4: return 'Severe'\n",
        "        else: return 'Critical'\n",
        "\n",
        "    processed['severity'] = processed['stage'].apply(get_severity)\n",
        "\n",
        "    return processed\n",
        "\n",
        "# Process the data - use the correct variable name\n",
        "# Uses above feature engineering method\n",
        "processed_data = process_data(loadshedding_df)  # Changed from loadshedding_data to loadshedding_df\n",
        "\n",
        "print(\"ðŸ”„ Data processed successfully!\")\n",
        "print(f\"ðŸ“Š Shape: {processed_data.shape}\")\n",
        "print(\"\\nðŸ“‹ New features:\")\n",
        "new_cols = [c for c in processed_data.columns if c not in loadshedding_df.columns]  # Also updated here\n",
        "print(new_cols)\n",
        "\n",
        "print(\"\\nðŸ“Š Sample processed data:\")\n",
        "print(processed_data[['created_at', 'stage', 'season', 'severity', 'is_active']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qX6bJiwIKeu"
      },
      "outputs": [],
      "source": [
        "# Upload processed data to S3\n",
        "# Q10 Why?\n",
        "processed_path = s3_mgr.upload_df(processed_data, f'processed/loadshedding_{timestamp}.csv')\n",
        "print(f\"ðŸ“‹ Processed data path: {processed_path}\")\n",
        "\n",
        "print(\"\\nâœ… Data processing and upload completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbT45jl2IKeu"
      },
      "outputs": [],
      "source": [
        "# Data visualization and analysis\n",
        "# Q11 Why?\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('South Africa Load Shedding Analysis (2015-2020)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Events by year\n",
        "yearly = processed_data.groupby('year')['stage'].count()\n",
        "yearly.plot(kind='bar', ax=axes[0,0], color='red', alpha=0.7)\n",
        "axes[0,0].set_title('Load Shedding Events by Year')\n",
        "axes[0,0].set_ylabel('Number of Events')\n",
        "\n",
        "# 2. Stage distribution\n",
        "stage_dist = processed_data['stage'].value_counts().sort_index()\n",
        "stage_dist.plot(kind='bar', ax=axes[0,1], color='orange', alpha=0.7)\n",
        "axes[0,1].set_title('Distribution by Stage')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "\n",
        "# 3. Monthly patterns\n",
        "monthly = processed_data.groupby('month')['stage'].count()\n",
        "monthly.plot(kind='bar', ax=axes[1,0], color='blue', alpha=0.7)\n",
        "axes[1,0].set_title('Events by Month')\n",
        "axes[1,0].set_ylabel('Number of Events')\n",
        "\n",
        "# 4. Severity over time\n",
        "monthly_severity = processed_data.groupby(['year', 'month'])['stage'].mean().reset_index()\n",
        "monthly_severity['date'] = pd.to_datetime(monthly_severity[['year', 'month']].assign(day=1))\n",
        "axes[1,1].plot(monthly_severity['date'], monthly_severity['stage'], marker='o', color='green')\n",
        "axes[1,1].set_title('Average Stage Over Time')\n",
        "axes[1,1].set_ylabel('Average Stage')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Key insights\n",
        "print('ðŸ” Key Insights:')\n",
        "print(f'ðŸ“Š Total events: {len(processed_data):,}')\n",
        "print(f'âš¡ Active events: {processed_data[\"is_active\"].sum():,}')\n",
        "print(f'ðŸ”´ Max stage: {processed_data[\"stage\"].max()}')\n",
        "print(f'ðŸ“ˆ Peak year: {yearly.idxmax()} ({yearly.max()} events)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scLOsZlmIKeu"
      },
      "outputs": [],
      "source": [
        "# Machine Learning Model\n",
        "# Q12 Why?\n",
        "# Q13 Read the below code and explain what is does\n",
        "print('ðŸ¤– Training ML model...')\n",
        "\n",
        "# Creates a list of column names that will be used as input features (predictors) for the model\n",
        "# These are the variables the model will use to predict the load shedding stage\n",
        "features = ['year', 'month', 'day', 'hour', 'day_of_week', 'quarter', 'is_weekend', 'is_business_hours']\n",
        "X = processed_data[features]\n",
        "y = processed_data['stage']\n",
        "\n",
        "\n",
        "# Splits the data into training and testing sets:\n",
        "# - 80% for training the model (X_train, y_train)\n",
        "# - 20% for testing the model's performance (X_test, y_test)\n",
        "# - random_state=42 ensures reproducible results (same split every time)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Creates a StandardScaler object that will normalize features to have mean=0 and std=1\n",
        "# This ensures all features are on the same scale (important for many ML algorithms)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "# Creates a Random Forest model with:\n",
        "# - n_estimators=100: Uses 100 decision trees\n",
        "# - random_state=42: Ensures reproducible results\n",
        "# - RandomForestRegressor: Used for predicting continuous values (load shedding stages)\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# Trains the model using the scaled training features (X_train_scaled) and targets (y_train)\n",
        "# The model learns patterns between features and load shedding stages\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "# Uses the trained model to predict load shedding stages for the test data\n",
        "# These predictions will be compared to actual values to evaluate performance\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "# Calculates Mean Squared Error: average of squared differences between actual and predicted values\n",
        "# Lower MSE = better model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Displays the calculated performance metrics\n",
        "# .4f formats numbers to 4 decimal places\n",
        "print(f'ðŸ“Š Model Performance:')\n",
        "print(f'  MSE: {mse:.4f}')\n",
        "print(f'  RÂ²: {r2:.4f}')\n",
        "\n",
        "\n",
        "# Feature importance\n",
        "# Creates a DataFrame showing which features are most important for predictions\n",
        "# Random Forest provides feature_importances_ showing how much each feature contributes\n",
        "# sort_values orders features from most to least important\n",
        "importance = pd.DataFrame({\n",
        "    'feature': features,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print('\\nðŸ“ˆ Feature Importance:')\n",
        "for _, row in importance.iterrows():\n",
        "    print(f'  {row[\"feature\"]}: {row[\"importance\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zi7oLc4IKeu"
      },
      "outputs": [],
      "source": [
        "# Test S3 data retrieval\n",
        "print('ðŸ“¥ Testing S3 data retrieval...')\n",
        "\n",
        "# Download data from S3\n",
        "s3_data = s3_mgr.download_df(f'processed/loadshedding_{timestamp}.csv')\n",
        "\n",
        "print(f'âœ… Retrieved {len(s3_data)} rows from S3')\n",
        "print('Sample:')\n",
        "print(s3_data.head(3))\n",
        "\n",
        "# Data integrity check\n",
        "match = processed_data.equals(s3_data)\n",
        "print(f'\\nâœ… Data integrity: {\"PASSED\" if match else \"FAILED\"}')\n",
        "\n",
        "# List all files\n",
        "print('\\nðŸ“ All S3 files:')\n",
        "s3_mgr.list_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG0DtPtwIKev"
      },
      "outputs": [],
      "source": [
        "# Interactive visualization\n",
        "print('ðŸ“Š Creating interactive charts...')\n",
        "\n",
        "# Time series plot\n",
        "fig = px.line(\n",
        "    monthly_severity,\n",
        "    x='date',\n",
        "    y='stage',\n",
        "    title='Load Shedding Severity Over Time',\n",
        "    labels={'stage': 'Average Stage', 'date': 'Date'}\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Heatmap by hour and day\n",
        "heatmap_data = processed_data.groupby(['hour', 'day_of_week'])['stage'].mean().reset_index()\n",
        "heatmap_pivot = heatmap_data.pivot(index='hour', columns='day_of_week', values='stage')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(heatmap_pivot, annot=True, cmap='Reds', fmt='.2f')\n",
        "plt.title('Average Load Shedding by Hour and Day')\n",
        "plt.xlabel('Day (0=Monday, 6=Sunday)')\n",
        "plt.ylabel('Hour')\n",
        "plt.show()\n",
        "\n",
        "# Seasonal analysis\n",
        "seasonal = processed_data.groupby('season').agg({\n",
        "    'stage': ['mean', 'max', 'count'],\n",
        "    'is_active': 'sum'\n",
        "}).round(2)\n",
        "\n",
        "print('ðŸŒ Seasonal Analysis:')\n",
        "print(seasonal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCbvxmslIKev"
      },
      "outputs": [],
      "source": [
        "# Save model metrics to S3\n",
        "print('ðŸ’¾ Saving model artifacts...')\n",
        "\n",
        "metrics = {\n",
        "    'timestamp': timestamp,\n",
        "    'model': 'RandomForestRegressor',\n",
        "    'mse': float(mse),\n",
        "    'r2': float(r2),\n",
        "    'features': features,\n",
        "    'train_size': len(X_train),\n",
        "    'test_size': len(X_test)\n",
        "}\n",
        "\n",
        "# Upload metrics\n",
        "metrics_json = json.dumps(metrics, indent=2)\n",
        "try:\n",
        "    aws.s3_client.put_object(\n",
        "        Bucket=aws.bucket_name,\n",
        "        Key=f'models/metrics_{timestamp}.json',\n",
        "        Body=metrics_json.encode('utf-8'),\n",
        "        ContentType='application/json'\n",
        "    )\n",
        "    print('âœ… Model metrics saved to S3')\n",
        "except Exception as e:\n",
        "    print(f'âŒ Save failed: {e}')\n",
        "\n",
        "print('\\nðŸŽ‰ Analysis complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhLJRkAhIKev"
      },
      "outputs": [],
      "source": [
        "# Executive Summary\n",
        "print('ðŸ“Š EXECUTIVE SUMMARY')\n",
        "print('='*50)\n",
        "\n",
        "total = len(processed_data)\n",
        "active = processed_data['is_active'].sum()\n",
        "max_stage = processed_data['stage'].max()\n",
        "date_range = f\"{processed_data['created_at'].min().year}-{processed_data['created_at'].max().year}\"\n",
        "\n",
        "print('ðŸ” KEY FINDINGS:')\n",
        "print(f'  ðŸ“Š Total events: {total:,}')\n",
        "print(f'  âš¡ Active events: {active:,}')\n",
        "print(f'  ðŸ”´ Max stage: {max_stage}')\n",
        "print(f'  ðŸ“… Period: {date_range}')\n",
        "print(f'  ðŸ“ˆ Peak year: {yearly.idxmax()}')\n",
        "print()\n",
        "\n",
        "print('â˜ï¸ AWS ACHIEVEMENTS:')\n",
        "print('  âœ… S3 data pipeline implemented')\n",
        "print('  âœ… Raw and processed data uploaded')\n",
        "print('  âœ… Bi-directional data flow demonstrated')\n",
        "print('  âœ… Data versioning with timestamps')\n",
        "print()\n",
        "\n",
        "print('ðŸ¤– ML INSIGHTS:')\n",
        "print(f'  ðŸ“ˆ RÂ² Score: {r2:.4f}')\n",
        "print(f'  ðŸŽ¯ Top feature: {importance.iloc[0][\"feature\"]}')\n",
        "print(f'  ðŸ“Š Training samples: {len(X_train)}')\n",
        "print()\n",
        "\n",
        "print('ðŸ’¡ RECOMMENDATIONS:')\n",
        "print('  ðŸ”‹ Focus on peak crisis periods')\n",
        "print('  â° Optimize maintenance timing')\n",
        "print('  ðŸ“Š Real-time monitoring system')\n",
        "print('  â˜ï¸ Scale AWS infrastructure')\n",
        "print()\n",
        "\n",
        "print('ðŸš€ NEXT STEPS:')\n",
        "print('  1. AWS Lambda automation')\n",
        "print('  2. QuickSight dashboards')\n",
        "print('  3. SageMaker deployment')\n",
        "print('  4. CloudWatch alerts')\n",
        "print()\n",
        "\n",
        "print('âœ… NOTEBOOK READY FOR GOOGLE COLAB!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Homework - next Steps\n",
        "Well Architected data pipeline\n",
        "\n",
        "Daigram here\n",
        "\n",
        "Training Job\n",
        "S3 -> Sagemake Processing Job -> Sagamate training -> Model -> Deploy model\n",
        "\n",
        "Inference Job:\n",
        "S3 -> Sagemake Processing Job -> Inference -> Store Data in S3 -> Create Visualisation in QS\n",
        "\n",
        "#Thank you!!"
      ],
      "metadata": {
        "id": "Zdh99E3eYrj9"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}